---
layout: ../layouts/Layout.astro
title: HΩ
---

<div class="hero">

# HΩ

<div class="purpose-statement">We advance AI safety <span class="purpose-detail">through collaboration, research, and education.</span></div>

<div class="cta-group">
  <a href="#get-involved" class="btn">Get involved</a>
</div>

</div>

<section id="what-we-do">

## What we do

<div class="editorial-grid">

<div class="editorial-item">
<span class="editorial-meta">Since 2024 · 275+ subscribers</span>

**[Guaranteed Safe AI Seminars](https://luma.com/guaranteedsafeaiseminars)** Monthly technical seminars on quantitative and guaranteed safety approaches. Featuring leading researchers including Yoshua Bengio, Steve Omohundro, Tan Zhi Xuan, and Jobst Heitzig.

</div>

<div class="editorial-item">
<span class="editorial-meta">Since 2025 · 1600+ members</span>

**[AI Safety in Montréal](https://aisafetymontreal.org)** Local field‑building hub serving the Montréal AI safety, ethics & governance community. Meetups, coworking sessions, targeted workshops, advising, and collaborations.

</div>

<div class="editorial-item">
<span class="editorial-meta">Since 2025</span>

**[Canadian AI Safety Coordination](mailto:team@horizonomega.org?subject=Canadian%20AI%20Safety%20Treaty)** Coordination group between Canadian orgs and network working towards AI safety.

</div>

<div class="editorial-item">
<span class="editorial-meta">Since 2018 · 400+ researchers</span>

**[AI Safety Unconference](https://www.aisafetyunconference.org/)** Participant-driven events featuring talks, sessions, moderated discussions, and 1:1s. Organized AI Safety Unconference @ NeurIPS (2018–2022), Virtual AI Safety Unconference (2024), and hybrid edition planned for 2026.

</div>

<div class="editorial-item">
<span class="editorial-meta">Since 2024</span>

**[Horizon Events](https://www.horizonevents.info/)** Curating multiple global AI safety event series. Supporting the broader ecosystem of AI safety events and initiatives.

</div>

</div>

</section>

<section id="track-record">

## Track record

We've engaged thousands of participants across events and channels since 2018, building a community now with 1600+ members in Montréal, and facilitated cross-organizational collaboration globally.

<div class="editorial-grid">

<div class="editorial-item">
<span class="editorial-meta">2018–2022</span>

**[AI Safety Unconference @ NeurIPS](https://www.horizonevents.info/events/ai-safety-unconference)** Years of participant-driven convenings featuring lightning talks, moderated discussions, and 1:1 sessions. 60+ participants per event from leading organizations including Anthropic, DeepMind, OpenAI, Mila, MIRI, MIT, Stanford, Oxford, Cambridge, and more.

</div>

<div class="editorial-item">
<span class="editorial-meta">2024–present</span>

**[Guaranteed Safe AI Seminars](https://luma.com/guaranteedsafeaiseminars)** Monthly technical talks with 15–30 live attendees per session, and 600+ total registrations annually. Featured speakers include Yoshua Bengio, Steve Omohundro, Tan Zhi Xuan, and Jobst Heitzig.

</div>

<div class="editorial-item">
<span class="editorial-meta">2025–present</span>

**[Montréal AI safety community](https://aisafetymontreal.org/)** Built local ecosystem through meetups, coworking sessions, targeted workshops, and co-running the Mila AI safety reading group (biweekly sessions with 10–20 researchers). Serving members across AI safety, ethics, and governance.

</div>

<div class="editorial-item">
<span class="editorial-meta">2022, 2025</span>

**[Limits to Control workshop](http://limitstocontrol.org)** Co-organized workshop focused on difficulties or impossibilities of controlling advanced AI systems. The 2025 edition brought together researchers to map the territory of AI control limitations and produced a collective statement.

</div>

<div class="editorial-item">
<span class="editorial-meta">2023–present</span>

**[AI Safety Events & Training newsletter](https://aisafetyeventsandtraining.substack.com/)** Founded its Substack in 2023, contributing events curation and community growth support.

</div>

</div>

<div class="testimonials">

### What participants say

<div class="testimonials-grid">

<div class="testimonial">
Helpful for keeping up with the cutting edge and for launching collaborations.
<cite>Haydn Belfield, Google DeepMind</cite>
</div>

<div class="testimonial">
Very useful to meet and talk with AI safety researchers at NeurIPS.
<cite>Esben Kran, Apart Research</cite>
</div>

<div class="testimonial">
A great way to meet the best people in the area and propel daring ideas forward.
<cite>Stuart Armstrong, Aligned AI</cite>
</div>

<div class="testimonial">
Small discussion groups exposed me to new perspectives.
<cite>Adam Gleave, FAR.AI</cite>
</div>

</div>

</div>

</section>

<section id="get-involved">

## Get involved

<div class="editorial-grid">

<div class="editorial-item">

**[Volunteer](mailto:team@horizonomega.org?subject=Volunteering)** We welcome volunteers for seminar operations, research briefs, speaker outreach, and video editing. Training and templates provided.

</div>

<div class="editorial-item">

**[Partner](mailto:team@horizonomega.org?subject=Partnership)** We collaborate with universities, labs, NGOs, and standards bodies to co‑host sessions, share speakers, and to build pilots.

</div>

<div class="editorial-item">

**[Support](mailto:team@horizonomega.org?subject=Support)** Your support enables us to expand collaboration, events, and research. Sponsorships, grants, and in‑kind contributions (venue hosting, captioning, editing, design) welcome.

</div>

<div class="editorial-item">

**[Advisors](mailto:team@horizonomega.org?subject=Advisors)** Seeking senior advisors across verification, evaluations, and governance. Conflict of interest policy applies.

</div>

<div class="editorial-item">

**[Follow](https://horizonomega.substack.com/)** Stay updated on AI safety events, training opportunities, and our latest initiatives. Subscribe to our newsletter and follow us on social media.

</div>

</div>

</section>

<section id="acknowledgements">

## Acknowledgements

### Contributors

<div class="acknowledgements-table">

| | |
|---|---|
| [Orpheus Lummis](https://www.orpheuslummis.info/) | Founder |
| [Étienne Langlois](https://ca.linkedin.com/in/%C3%A9tienne-langlois-2a3119294/en) | AI safety coordination & strategy |
| [Linda Linsefors](https://www.lesswrong.com/users/linda-linsefors) | Advisor, events & AI safety |
| [Arjun Yadav](https://www.arjunyadav.net/) | Generalist support & events |
| [Manu García](https://www.linkedin.com/in/manu-garcía-communications-specialist) | Communications specialist & event coordinator |
| [Pascal Huynh](https://www.facebook.com/pozcal) | Event & interaction design |
| [Nicolas Grenier](https://en.wikipedia.org/wiki/Nicolas_Grenier_(artist)) | Advisor, worlding |
| [Richard Mallah](https://www.linkedin.com/in/richardmallah/) | Advisor, AI Safety Unconference series |
| [Mario Gibney](https://www.linkedin.com/in/mario-gibney-08bb7b45/) | Advisor, AI safety field-building |
| [Diego Jiménez](https://co.linkedin.com/in/diegomauriciojimenez/en) | AI strategy & events operations |
| [Vaughn DiMarco](https://va.ug.hn/) | Advisor, AI Safety Unconference series |
| [David Krueger](https://davidscottkrueger.com/) | Co-organizer, AI Safety Unconference series |

</div>

### Funders & sponsors

<div class="acknowledgements-table">

| |
|---|
| [Long‑Term Future Fund (EA Funds)](https://funds.effectivealtruism.org/funds/far-future) |
| [Survival and Flourishing Fund](https://survivalandflourishing.fund/) |
| [Effective Altruism Foundation](https://ea-foundation.org/) |
| [Future of Life Institute](https://futureoflife.org/) |

</div>

</section>

<section id="contact">

## Contact

We'd love to hear from you. Reach out to discuss collaborations, ask questions, or explore opportunities to work together on AI safety initiatives.

<a href="mailto:team@horizonomega.org" class="btn">Email us</a>
<a href="https://cal.com/horizonomega" class="btn">Schedule a call</a>

</section>
