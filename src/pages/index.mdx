---
layout: ../layouts/Layout.astro
title: HΩ
---

<div class="hero">

# HΩ

<div class="purpose-statement">The flourishing of sentient life by reducing risks from AI.</div>

<p class="mission-statement">We advance AI safety through collaboration, research, and education.</p>

<div class="cta-group">
  <a href="#get-involved" class="btn btn-primary">Get involved</a>
</div>

</div>

<section id="what-we-do">

## What we do

<div class="card-grid">

<div class="card">
  <div class="card-header">
    <h3>Guaranteed Safe AI Seminars</h3>
    <span class="card-meta">Since 2024</span>
  </div>
  <p>Monthly technical seminars on quantitative and guaranteed safety approaches. Featuring leading researchers including Yoshua Bengio, Steve Omohundro, Tan Zhi Xuan, and Jobst Heitzig.</p>
  <div class="card-stats">
    <span>275+ subscribers</span>
  </div>
  <a href="https://luma.com/guaranteedsafeaiseminars" class="card-link">Events calendar →</a>
</div>

<div class="card">
  <div class="card-header">
    <h3>AI Safety in Montréal</h3>
    <span class="card-meta">Since 2025</span>
  </div>
  <p>Local field‑building hub serving the Montréal AI safety, ethics & governance community. Meetups, coworking sessions, targeted workshops, advising, and collaborations.</p>
  <div class="card-stats">
    <span>1600+ members</span>
  </div>
  <a href="https://aisafetymontreal.org" class="card-link">Visit site →</a>
</div>

<div class="card">
  <div class="card-header">
    <h3>Coordination towards International AI Safety Treaty</h3>
    <span class="card-meta">Since 2025</span>
  </div>
  <p>Monthly coordination calls bringing together Canadian actors across civil society, research institutions, standards bodies, and industry to advance coordinated AI safety policy and initiatives.</p>
  <a href="mailto:team@horizonomega.org?subject=Canadian%20AI%20Safety%20Treaty" class="card-link">Contact us →</a>
</div>

<div class="card">
  <div class="card-header">
    <h3>AI Safety Unconference</h3>
    <span class="card-meta">2018–2022, 2024</span>
  </div>
  <p>Participant-driven events featuring talks, sessions, moderated discussions, and 1:1s. Organized AI Safety Unconference @ NeurIPS (2018–2022) and Virtual AI Safety Unconference (2024).</p>
  <div class="card-stats">
    <span>400+ researchers convened</span>
  </div>
  <a href="https://www.horizonevents.info/" class="card-link">Learn more →</a>
</div>

<div class="card">
  <div class="card-header">
    <h3>Horizon Events</h3>
    <span class="card-meta">Since 2024</span>
  </div>
  <p>Curating multiple global AI safety event series. Supporting the broader ecosystem of AI safety events and initiatives.</p>
  <a href="https://www.horizonevents.info/" class="card-link">Visit site →</a>
</div>

<div class="card">
  <div class="card-header">
    <h3>Propose a Collaboration</h3>
    <span class="card-meta">Open</span>
  </div>
  <p>Have an idea for a collaborative project? We're interested in joint initiatives, research partnerships, event co-hosting, and cross-organizational coordination in AI safety.</p>
  <a href="mailto:team@horizonomega.org?subject=Collaborative%20Project%20Proposal" class="card-link">Propose project →</a>
</div>

</div>

</section>

<section id="track-record">

## Track record

We've engaged thousands of participants across events and channels since 2018, established a 1600+ members community in Montréal, and facilitated cross-organizational collaboration globally.

**[AI Safety Unconference @ NeurIPS](https://www.horizonevents.info/events/ai-safety-unconference) (2018–2022).** Years of participant-driven convenings featuring lightning talks, moderated discussions, and 1:1 sessions. 60+ participants per event from leading organizations including Anthropic, DeepMind, OpenAI, Mila, MIRI, MIT, Stanford, Oxford, Cambridge, and more.

**[Guaranteed Safe AI Seminars](https://luma.com/guaranteedsafeaiseminars) (2024–present).** Monthly technical talks with 15–30 live attendees per session, and 600+ total registrations annually. Featured speakers include Yoshua Bengio, Steve Omohundro, Tan Zhi Xuan, and Jobst Heitzig.

**[Montréal AI safety community](https://aisafetymontreal.org/) (2025–present).** Built local ecosystem through meetups, coworking sessions, targeted workshops, and co-running the Mila AI safety reading group (biweekly sessions with 10–20 researchers). Serving members across AI safety, ethics, and governance.

**[Limits to Control workshop](http://limitstocontrol.org).** Co-organized workshop focused on difficulties or impossibilities of controlling advanced AI systems.

**[AI Safety Events & Training newsletter](https://aisafetyeventsandtraining.substack.com/).** Founded its Substack in 2023, contributing events curation and community growth support.

<details>
  <summary>What participants say</summary>
  <blockquote>"Helpful for keeping up with the cutting edge and for launching collaborations." — Haydn Belfield</blockquote>
  <blockquote>"Very useful to meet and talk with AI safety researchers at NeurIPS." — <strong>Esben Kran</strong></blockquote>
  <blockquote>"A great way to meet the best people in the area and propel daring ideas forward." — <strong>Stuart Armstrong</strong></blockquote>
  <blockquote>"Small discussion groups exposed me to new perspectives." — <strong>Adam Gleave</strong></blockquote>
</details>

</section>

<section id="get-involved">

## Get involved

<div class="card-grid">

<div class="card">
  <h3>Volunteer</h3>
  <p>We welcome volunteers for seminar operations, research briefs, speaker outreach, and video editing. Training and templates provided.</p>
  <a href="mailto:team@horizonomega.org?subject=Volunteering" class="card-link">Email us →</a>
</div>

<div class="card">
  <h3>Partner</h3>
  <p>We collaborate with universities, labs, NGOs, and standards bodies to co‑host sessions, share speakers, and to build pilots.</p>
  <a href="mailto:team@horizonomega.org?subject=Partnership" class="card-link">Email us →</a>
</div>

<div class="card">
  <h3>Support</h3>
  <p>Your support enables us to expand collaboration, events, and research. Sponsorships, grants, and in‑kind contributions (venue hosting, captioning, editing, design) welcome.</p>
  <a href="mailto:team@horizonomega.org?subject=Support" class="card-link">Discuss →</a>
</div>

<div class="card">
  <h3>Advisors</h3>
  <p>Seeking senior advisors across verification, evaluations, and governance. Conflict of interest policy applies.</p>
  <a href="mailto:team@horizonomega.org?subject=Advisors" class="card-link">Contact us →</a>
</div>

<div class="card">
  <h3>Follow</h3>
  <p>Stay updated on AI safety events, training opportunities, and our latest initiatives. Subscribe to our newsletter and follow us on social media.</p>
  <a href="https://horizonomega.substack.com/" class="card-link">Subscribe →</a>
</div>

</div>

</section>

<section id="acknowledgements">

## Acknowledgements

**Contributors**
- [Orpheus Lummis](https://www.orpheuslummis.info/) — Founder
- [Étienne Langlois](https://ca.linkedin.com/in/%C3%A9tienne-langlois-2a3119294/en) – AI safety coordination & strategy
- [Linda Linsefors](https://www.lesswrong.com/users/linda-linsefors) — Advisor, events & AI safety
- [Arjun Yadav](https://www.arjunyadav.net/) — Generalist support & events
- [Manu García](https://www.linkedin.com/in/manu-garcía-communications-specialist) — Communications specialist & event coordinator
- [Pascal Huynh](https://www.facebook.com/pozcal) — Event & interaction design
- [Nicolas Grenier](https://en.wikipedia.org/wiki/Nicolas_Grenier_(artist)) — Advisor, worlding
- [Richard Mallah](https://www.linkedin.com/in/richardmallah/) — Advisor, AI Safety Unconference series
- [Diego Jiménez](https://co.linkedin.com/in/diegomauriciojimenez/en) — AI strategy & events operations
- [Vaughn DiMarco](https://va.ug.hn/) — Advisor, AI Safety Unconference series
- [David Krueger](https://davidscottkrueger.com/) — Co-organizer, AI Safety Unconference series

**Funders & sponsors**
- [Long‑Term Future Fund (EA Funds)](https://funds.effectivealtruism.org/funds/far-future)
- [Survival and Flourishing Fund](https://survivalandflourishing.fund/)
- [Effective Altruism Foundation](https://ea-foundation.org/)
- [Future of Life Institute](https://futureoflife.org/)

</section>

<section id="contact">

## Contact

<p>We'd love to hear from you. Reach out to discuss collaborations, ask questions, or explore opportunities to work together on AI safety initiatives.</p>

<div class="cta-group">
  <a href="mailto:team@horizonomega.org" class="btn btn-primary">Email us</a>
  <a href="https://cal.com/horizonomega" class="btn btn-secondary">Schedule a call</a>
</div>

</section>
